{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score,f1_score,classification_report,precision_score,recall_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "from string import digits\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 190234 entries, 0 to 190233\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   Unnamed: 0      190234 non-null  int64 \n",
      " 1   Id              190234 non-null  int64 \n",
      " 2   Title           190234 non-null  object\n",
      " 3   QuestionBody    190234 non-null  object\n",
      " 4   AnswerBody      190234 non-null  object\n",
      " 5   QuestionTime    190234 non-null  object\n",
      " 6   AnswerTime      190234 non-null  object\n",
      " 7   AnswerTimeDiff  190234 non-null  int64 \n",
      " 8   Tags            190234 non-null  object\n",
      " 9   UserId          190234 non-null  int64 \n",
      " 10  UserReputation  190234 non-null  int64 \n",
      " 11  UserPageViews   190234 non-null  int64 \n",
      " 12  UserUpVote      190234 non-null  int64 \n",
      " 13  UserDownVotes   190234 non-null  int64 \n",
      " 14  BadgeNum        190234 non-null  int64 \n",
      " 15  Q_time_hr       190234 non-null  int64 \n",
      " 16  Q_time_weekday  190234 non-null  int64 \n",
      " 17  Q_Range         190234 non-null  object\n",
      " 18  Workday_class   190234 non-null  object\n",
      "dtypes: int64(11), object(8)\n",
      "memory usage: 27.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('python_data .csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en = spacy.load('en_core_web_sm')\n",
    "#sw_spacy = en.Defaults.stop_words\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords_removed = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for digit removing\n",
    "remove_digits = str.maketrans('', '', digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for punctuation removing\n",
    "remove_punkt = str.maketrans(string.punctuation,' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemmatized_text(text):\n",
    "    lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(df_text):\n",
    "    #  lower case\n",
    "    df_text = df_text.str.lower()\n",
    "    \n",
    "    #  Decode html\n",
    "    df_text = df_text.apply(unescape)\n",
    "    \n",
    "    #  Remove html\n",
    "    df_text = df_text.apply(lambda x: remove_html(x))\n",
    "    \n",
    "    #  Remove stopwords\n",
    "    df_text = df_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    #  Remove digits\n",
    "    df_text = df_text.apply(lambda x: x.translate(remove_digits))\n",
    "    \n",
    "    #  Remove punctuation\n",
    "    df_text = df_text.apply(lambda x: x.translate(remove_punkt))\n",
    "    \n",
    "    #  Lemmatization\n",
    "    df_text_processed = df_text.apply(lambda x: get_lemmatized_text(x))\n",
    "    \n",
    "    return df_text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_processing(text):\n",
    "    #lower case\n",
    "    text = text.lower()\n",
    "    #remove html\n",
    "    text = unescape(text)\n",
    "    text = remove_html(text)\n",
    "    #remove stopwords \n",
    "    text = remove_stopwords(text)\n",
    "    #remove digits \n",
    "    text = text.translate(remove_digits)\n",
    "    #remove punctuation \n",
    "    text = text.translate(remove_punkt)\n",
    "    #Lemmatization\n",
    "    text = get_lemmatized_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(tags):\n",
    "    #clean tags\n",
    "    tags = re.sub(r'[,<>.ï¼Ÿ:]', ' ', tags)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text \n",
    "data['clean_question'] = text_processing(data['QuestionBody'])\n",
    "data['clean_title'] = text_processing(data['Title'])\n",
    "data['clean_Tags'] = data['Tags'].apply(clean_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get trial question \n",
    "tag = data['Tags'][10]\n",
    "title = data['Title'][10]\n",
    "question = data['QuestionBody'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering with SimTitle and SimTag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Calculate the similarity from title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tf-idf vectorizer for title \n",
    "tf_title = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0,max_features =500)\n",
    "tfidf_title = tf_title.fit_transform(data['clean_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_title(title):\n",
    "    #cleaning the input \n",
    "    title = trial_processing(title)\n",
    "    title_asSeries = pd.Series(title)\n",
    "    #transform the string input into a matrix \n",
    "    title_matrix = tf_title.transform(title_asSeries)\n",
    "\n",
    "    title_trial1 = title_matrix.tocsr().todense()\n",
    "    #calculate the cosine similarity \n",
    "    cosine_similarities = cosine_similarity(title_trial1, tfidf_title)\n",
    "    #get index \n",
    "    similarity_indices = cosine_similarities[0].argsort()[::-1]\n",
    "    #ger id and cosine similarity \n",
    "    similar_items = [(cosine_similarities[0][i], data['Id'][i]) for i in similarity_indices]\n",
    "\n",
    "\n",
    "    result_title = {}\n",
    "    #put id and cosine similarity in the dict \n",
    "    for i in similar_items:\n",
    "        result_title[i[1]] = i[0]\n",
    "    \n",
    "    \n",
    "    return result_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_title = sim_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Calculate the similarity from tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tf-idf vectorizer for tag\n",
    "tag_vectorizer = TfidfVectorizer()\n",
    "count_matrix = tag_vectorizer.fit_transform(data['clean_Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_tags(tags):\n",
    "    #clean tags \n",
    "    tags = clean_tags(tags)\n",
    "    tags_asSeries = pd.Series(tags)\n",
    "    #transform the string input into a matrix \n",
    "    tags_matrix = tag_vectorizer.transform(tags_asSeries)\n",
    "\n",
    "    tag_trial1 = tags_matrix.tocsr().todense()\n",
    "    #calculate the cosine similarity \n",
    "    cosine_similarities_tag = cosine_similarity(tag_trial1, count_matrix)\n",
    "    #get index\n",
    "    similarity_indices_tag = cosine_similarities_tag[0].argsort()[::-1]\n",
    "    #ger id and cosine similarity \n",
    "    similar_items_tag = [(cosine_similarities_tag[0][i], data['Id'][i]) for i in similarity_indices_tag]\n",
    "\n",
    "    #put id and cosine similarity in the dict \n",
    "    result_tag = {}\n",
    "\n",
    "    for i in similar_items_tag:\n",
    "        result_tag[i[1]] = i[0]\n",
    "        \n",
    "    return result_tag\n",
    "\n",
    "\n",
    "#result_tag\n",
    "result_tag = sim_tags(tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Filtering with threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2797"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_score(result_title, result_tags):\n",
    "    #set alpha to be 0.9\n",
    "    alpha = 0.9\n",
    "\n",
    "    result_filter = {}\n",
    "    #get title score and tag score\n",
    "    for i in result_title.keys():\n",
    "        title_score = result_title.get(i)\n",
    "        tag_score = result_tag.get(i)\n",
    "        #calculate the combined score \n",
    "        sim_score = alpha* title_score + (1-alpha)*tag_score\n",
    "        #put it in the dict with id \n",
    "        result_filter[i] = sim_score\n",
    "    results_ID = []\n",
    "    #filter with threshold 0.2 \n",
    "    for i in result_filter.keys():\n",
    "        if result_filter[i] >= 0.2:\n",
    "            results_ID.append(i)\n",
    "    return results_ID     \n",
    "        \n",
    "    \n",
    "result_ID = filter_score(result_title, result_tag)\n",
    "len(result_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4  Get Clean Question Body for each ID after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_question (result_ID):\n",
    "    result_text = {}\n",
    "\n",
    "    for i in result_ID :\n",
    "        #get filtered and cleaned question body by id \n",
    "        text = data.loc[data['Id'] == i]['clean_question'].values[0]\n",
    "        #get index\n",
    "        index = int(data.loc[data['Id'] == i].index.values)\n",
    "        result_text[index] = text\n",
    "\n",
    "    #transform to series \n",
    "    ser = pd.Series(data = result_text)\n",
    "    return ser\n",
    "\n",
    "clean_question = get_clean_question (result_ID)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Final Recommendation based on content of question body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get question body values \n",
    "def item(id):\n",
    "    return data.loc[data['Id'] == id]['QuestionBody'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend (question, num):\n",
    "    #fit a TF-IDF vectorizer \n",
    "    tf_question_body = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0,max_features =500)\n",
    "    #transform to matrixs \n",
    "    tfidf_matrix = tf_question_body.fit_transform(clean_question)\n",
    "    #clean input question\n",
    "    Question_body_clean = trial_processing(question)\n",
    "    \n",
    "    QB_asSeries = pd.Series(Question_body_clean)\n",
    "    #get transformed matrix \n",
    "    QB_matrix = tf_question_body.transform(QB_asSeries)\n",
    "\n",
    "    QB_trial1 = QB_matrix.tocsr().todense()\n",
    "    #calculate the similarity score \n",
    "    cosine_similarities = cosine_similarity(QB_trial1, tfidf_matrix)\n",
    "    #return index of 50 question with higest score\n",
    "    similarity_indices = cosine_similarities[0].argsort()[:-num-2:-1]\n",
    "    #get highest score \n",
    "    sim_score = [cosine_similarities[0][i] for i in similarity_indices]\n",
    "    #get index for raw dataset\n",
    "    sim_index = []\n",
    "    for i in similarity_indices:\n",
    "        sim_index.append(clean_question.index.values[i])\n",
    "    \n",
    "    #get ID \n",
    "    sim_id = [(data['Id'][i]) for i in sim_index]\n",
    "    \n",
    "    result_id = sim_id[1:num+1]\n",
    "    print(\"Input ID:\")\n",
    "    print(sim_id[0])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Recommend ID :\")\n",
    "    print(result_id)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Cosine Similarity Score:\")\n",
    "    print(sim_score[1:])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Recommending \" + str(num) + \" product similar to : \\n\" + Question_body_clean)\n",
    "    print('\\n')\n",
    "    for i in result_id:\n",
    "        print(\"Recommend: \" + str(item(i)))\n",
    "        print('\\n')\n",
    " \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID:\n",
      "53992768\n",
      "\n",
      "\n",
      "Recommend ID :\n",
      "[59559519, 54066612, 59641251, 61289172, 58134427]\n",
      "\n",
      "\n",
      "Cosine Similarity Score:\n",
      "[0.5545847373713634, 0.5457575415558475, 0.5431425606030436, 0.5420573306987583, 0.5419112769870617]\n",
      "\n",
      "\n",
      "Recommending 5 product similar to : \n",
      "data frame look like following df pd dataframe k one two k checking duplicate get boolean index df duplicated use filter df df duplicated show different result compare df drop duplicate additional row created result one\n",
      "\n",
      "\n",
      "Recommend: ['<p>I have a following dataframe - </p>\\n\\n<pre class=\"lang-py prettyprint-override\"><code>  print df\\n\\n  Name | Role   |\\n  Mark | Admin  |\\n  Mark | Admin. |\\n\\n  df = df.drop_duplicates()\\n  print df\\n\\n  Name | Role  |\\n  Mark | Admin |\\n  Mark | Admin. |\\n</code></pre>\\n\\n<p>I want to ignore any leading or preceding punctuations (full stop in this case) and drop duplicates.</p>\\n\\n<p>Expected output - </p>\\n\\n<pre class=\"lang-py prettyprint-override\"><code>  df = df.drop_duplicates()\\n  print df\\n\\n  Name | Role  |\\n  Mark | Admin |\\n</code></pre>\\n']\n",
      "\n",
      "\n",
      "Recommend: [\"<p>I'm trying to use a boolean array to subset a data frame. This works:</p>\\n\\n<pre><code>df = pd.DataFrame(\\n    [\\n        (0, 0, 1),\\n        (0, 1, 2),\\n        (0, 3, 20),\\n        (1, 0, 2),\\n        (1, 1, 1),\\n        (1, 2, 30),\\n    ],\\n    columns = ['s', 'j', 'q']\\n)\\n\\ndf[df['j'] == 0]\\ndf.loc[df['j'] == 0]\\n</code></pre>\\n\\n<p>However, the following fail:</p>\\n\\n<pre><code>df.set_index('s')[df['j'] == 0]\\ndf.set_index('s').loc[df['j'] == 0]\\n</code></pre>\\n\\n<p>I get every instance wehere <code>s</code> equals 0, not <code>j</code>. I've resorted to query (my condition is more complicated than literally <code>j == 0</code>, or I would use it directly):</p>\\n\\n<pre><code>df['sub'] = (df['j'] == 0)\\ndf.query('sub')\\n</code></pre>\\n\\n<p>Is there a way to do this without creating a temporary variable? Many thanks! Python 3.7 and pandas 0.23.4</p>\\n\\n<p><strong>EDIT</strong></p>\\n\\n<p>The issue with what I was doing was that the boolean series and the data frame have different indexes. The answer below details several ways to work around it, but I was for either of these two methods:</p>\\n\\n<pre><code>df.set_index('s')[(df['j'] == 0).values]\\n</code></pre>\\n\\n<p>or </p>\\n\\n<pre><code>df.set_index('s', inplace = True)\\ndf[df['j'] == 0]\\n</code></pre>\\n\"]\n",
      "\n",
      "\n",
      "Recommend: ['<p>I want to drop duplicates rows of either in columns <code>A</code> or <code>B</code> from the following <code>df</code>:</p>\\n\\n<pre><code>df = pd.DataFrame({\"A\":[1, 1, 2, 3, 4], \"B\": [2, 3, 7, 5, 5], \"C\": [1, 2, 3, 3, 4]})\\nprint(df)\\n\\n   A  B  C\\n0  1  2  1\\n1  1  3  2\\n2  2  7  3\\n3  3  5  3\\n4  4  5  4\\n</code></pre>\\n\\n<p>My expected output will like this:</p>\\n\\n<pre><code>   A  B  C\\n0  1  2  1\\n2  2  7  3\\n3  3  5  3\\n</code></pre>\\n\\n<p>Obviously <code>df.drop_duplicates(subset=[\\'A\\', \\'B\\'], keep=False)</code> will not generate what I want.</p>\\n\\n<p>The following code works, but a little bit long. Just wonder if there are other more concise solutions? Thank you.</p>\\n\\n<pre><code>df.drop_duplicates(\\'A\\', inplace=True)\\ndf.drop_duplicates(\\'B\\', inplace=True)\\nprint(df)\\n\\n   A  B  C\\n0  1  2  1\\n2  2  7  3\\n3  3  5  3\\n</code></pre>\\n']\n",
      "\n",
      "\n",
      "Recommend: [\"<p>Assume we have <code>df</code> and <code>df_drop</code>:</p>\\n\\n<pre><code>df = pd.DataFrame({'A': [1,2,3], 'B': [1,1,1]})\\ndf_drop = df[df.A==df.B]\\n</code></pre>\\n\\n<p>I want to delete <code>df_drop</code> from <code>df</code> without using the explicit conditions used when creating <code>df_drop</code>. I.e. I'm not after the solution <code>df[df.A!=df.B]</code>, but would like to, basically, take <code>df</code> minus <code>df_drop</code> somehow. Hopes this is clear enough. Otherwise happy to elaborate!</p>\\n\"]\n",
      "\n",
      "\n",
      "Recommend: ['<p>I was wondering if I can used pandas <code>.drop</code> method to drop rows when chaining methods to construct a data frame.</p>\\n\\n<p>Dropping rows is straight forward once the data frame exists:</p>\\n\\n<pre><code>import pandas as pd\\n\\ndf1 = pd.DataFrame({\\'A\\': [1, 2, 3], \\'B\\': [5, 4, 3]})\\nprint(df1)\\n\\n# drop the entries that match \"2\"\\ndf1 = df1[df1[\\'A\\'] !=2]\\nprint(df1)\\n</code></pre>\\n\\n<p>However, I would like to do this while I am creating the data frame:</p>\\n\\n<pre><code>df2 = (pd.DataFrame({\\'A\\': [1, 2, 3], \\'B\\': [5, 4, 3]})\\n        .rename(columns={\\'A\\': \\'AA\\'})\\n#        .drop(lambda x: x[\\'A\\']!=2)\\n        )\\nprint(df2)\\n</code></pre>\\n\\n<p>The commented line does not work, but maybe there is a correct way of doing this. Grateful for any input.</p>\\n']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend(question,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
